{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37db485d-fcec-4c20-943e-239a74a70cf4",
   "metadata": {},
   "source": [
    "# Homomorphic Encrypted LeNet-1\n",
    "This notebook will show a very practical example of running the famous LeNet-1 DL model directly on encrypted data.\n",
    "\n",
    "![scheme](HE_processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6b3e7-d428-48e4-b9db-3609ceb94b25",
   "metadata": {},
   "source": [
    "## Homomorphic encryption operations\n",
    "First of all, we will look at Pyfhel, a Python library which wraps SEAL, one of the most used frameworks for HE.\n",
    "Pyfhel supports the BFV scheme, so, it is the one that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c74dbc3-b890-4a87-9e8e-0094b9fc17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pyfhel obj at 0x7efe592d9bd0, [pk:Y, sk:Y, rtk:-, rlk:-, contx(p=65537, m=4096, base=2, sec=128, dig=64i.32f, batch=False)]>\n",
      "Expected sum: 125.028207448, decrypted sum: 125.02820744784549\n",
      "Expected sub: 129.286137812, decrypted sum: 129.28613781183958\n",
      "Expected mul: -270.7131931708334, decrypted sum: -270.7131931686308\n"
     ]
    }
   ],
   "source": [
    "from Pyfhel import Pyfhel, PyPtxt, PyCtxt\n",
    "\n",
    "HE = Pyfhel()\n",
    "HE.contextGen(p=65537, m=4096)\n",
    "HE.keyGen()\n",
    "\n",
    "print(HE)\n",
    "\n",
    "a = 127.15717263\n",
    "b = -2.128965182\n",
    "ctxt1 = HE.encryptFrac(a)\n",
    "ctxt2 = HE.encryptFrac(b)\n",
    "\n",
    "ctxtSum = ctxt1 + ctxt2\n",
    "ctxtSub = ctxt1 - ctxt2\n",
    "ctxtMul = ctxt1 * ctxt2\n",
    "\n",
    "resSum = HE.decryptFrac(ctxtSum)\n",
    "resSub = HE.decryptFrac(ctxtSub) \n",
    "resMul = HE.decryptFrac(ctxtMul)\n",
    "\n",
    "print(f\"Expected sum: {a+b}, decrypted sum: {resSum}\")\n",
    "print(f\"Expected sub: {a-b}, decrypted sum: {resSub}\")\n",
    "print(f\"Expected mul: {a*b}, decrypted sum: {resMul}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274ad1bb-c26e-4f29-9301-997c04f43bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "m1 = HE.encryptFrac(a)\n",
    "print(HE.noiseLevel(m1))\n",
    "\n",
    "m2 = HE.encodeFrac(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09497da-60be-4c1f-9d2a-9bd2b9390e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "54\n",
      "82\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "print(HE.noiseLevel(m1+m1))\n",
    "print(HE.noiseLevel(m1*m1))\n",
    "print(HE.noiseLevel(m1+m2))\n",
    "print(HE.noiseLevel(m1*m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d0840-41a8-417a-97db-faab642806d6",
   "metadata": {},
   "source": [
    "Before starting, let's note that:\n",
    "  1. We will use the fractional encoder to encode (and encrypt) the values in our examples. BFV was born for integers, so, CKKS should be used if the use case involves fractional values. However it is a more complex scheme, and for this example BFV is sufficient.\n",
    "  2. We will not use batching (also called *packing*). While batching can greatly speed up the computations, it introduces limitations which make the encrypted ML much more complex. For this example, we will encrypt/encode each number with a polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b17e-7644-4ce8-99a6-c1906758e2eb",
   "metadata": {},
   "source": [
    "## LeNet-1\n",
    "The LeNet-1 is a small CNN developed by LeCun et al. It is composed of 5 layers: a convolutional layer with 4 kernels of size 5x5 and tanh activation, an average pooling layer with kernel of size 2, another convolutional layer with 16 kernels of size 5x5 and tanh activation, another average pooling layer with kernel of size 2, and a fully connected layers with size 192x10. \n",
    "\n",
    "The highest value in the output tensor corresponds to the label LeNet-1 associated to the input image. \n",
    "\n",
    "For this tutorial we will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc122e5-a460-4a73-bed7-012b1105081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89940a02-261f-4fc7-98e7-cea0a70be181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcf92dc-4929-4adc-b97e-2a1bf946bfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%\n",
      "/home/ale/DatiLinux/Repos/Pyfhel/venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6641a660-43a4-4ed4-895a-ac7d1841dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def train_net(network, epochs, device):\n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader: # Get Batch\n",
    "            images, labels = batch \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            preds = network(images) # Pass Batch\n",
    "            loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # Calculate Gradients\n",
    "            optimizer.step() # Update Weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        \n",
    "def test_net(network, device):\n",
    "    network.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader: # Get Batch\n",
    "            images, labels = batch \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            preds = network(images) # Pass Batch\n",
    "            loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        accuracy = round(100. * (total_correct / len(test_loader.dataset)), 4)\n",
    "\n",
    "    return total_correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f74067-df4d-4b70-ae4c-ec6970c8baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True # If set to false, it will load models previously trained and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a4f779-4a5f-4a56-94c1-dba733942fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8480f18-5742-45a6-b4b7-5839b2f96763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    accuracies = []\n",
    "    for i in range(0, experiments):\n",
    "        LeNet1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(4, 12, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(192, 10),\n",
    "        )\n",
    "        \n",
    "        LeNet1.to(device)\n",
    "        train_net(LeNet1, 15, device)\n",
    "        acc = test_net(LeNet1, device)\n",
    "        accuracies.append(acc)\n",
    "#         torch.save(LeNet1, \"LeNet1.pt\")\n",
    "else:\n",
    "    LeNet1 = torch.load(\"LeNet1.pt\")\n",
    "    LeNet1.eval()\n",
    "    LeNet1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19fbca30-aa89-4d74-a476-3c9107fbf851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.9882700000000002\n",
      "Var: 9.140999999999878e-07\n"
     ]
    }
   ],
   "source": [
    "m = np.array(accuracies)\n",
    "print(f\"Mean accuracy on test set: {np.mean(m)}\")\n",
    "print(f\"Var: {np.var(m)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfbc9-38d0-41a7-a140-9606bfeff53f",
   "metadata": {},
   "source": [
    "## Approximating\n",
    "As we know, there are some operations that cannot be performed homomorphically on encrypted values. Most notably, these operations are division and comparison. It is possible to perform only linear functions.\n",
    "\n",
    "Consequently, in the LeNet-1 scheme we used, we can not use `tanh()`. This is because we cannot apply its non-linearities.\n",
    "\n",
    "\n",
    "One of the most common approach is to replace it with a simple polynomial function, for example a square layer (which simply performs $x \\rightarrow x^2$).\n",
    "\n",
    "We define the model with all the non-linearities removed **approximated**. This model can be re-trained, and it will be ready to be used on encrypted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52ea6449-1108-446f-ba2d-8170edcb641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 2)\n",
    "\n",
    "LeNet1_Approx = nn.Sequential(\n",
    "    nn.Conv2d(1, 4, kernel_size=5),\n",
    "    Square(),\n",
    "    nn.AvgPool2d(kernel_size=2),\n",
    "            \n",
    "    nn.Conv2d(4, 12, kernel_size=5),\n",
    "    Square(),\n",
    "    nn.AvgPool2d(kernel_size=2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(192, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ccaff6b-7cd5-49d2-9b5c-e8a881005fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    approx_accuracies = []\n",
    "    for i in range(0, experiments):\n",
    "        LeNet1_Approx = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            Square(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(4, 12, kernel_size=5),\n",
    "            Square(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(192, 10),\n",
    "        )\n",
    "        \n",
    "        LeNet1_Approx.to(device)\n",
    "        train_net(LeNet1_Approx, 15, device)\n",
    "        acc = test_net(LeNet1_Approx, device)\n",
    "        approx_accuracies.append(acc)\n",
    "        torch.save(LeNet1, \"LeNet1_Approx.pt\")\n",
    "\n",
    "else:\n",
    "    LeNet1_Approx = torch.load(\"LeNet1_Approx.pt\")\n",
    "    LeNet1_Approx.eval()\n",
    "    LeNet1_Approx.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5434fcf-e01c-40c8-9ea8-bc9086e8beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.98773\n",
      "Var: 2.024099999999944e-06\n"
     ]
    }
   ],
   "source": [
    "m = np.array(approx_accuracies)\n",
    "print(f\"Mean: {np.mean(m)}\")\n",
    "print(f\"Var: {np.var(m)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb96de6-7053-40ce-bb88-a1532270683f",
   "metadata": {},
   "source": [
    "We can see that replacing `tanh()` with `square()` did not impact the accuracy of the model dramatically. Usually this is not the case, and approximating DL models may worsen the performance badly. This is one of the challenges that HE-ML will have to consider: the creation of DL models keeping in mind the HE constraints from the beginning.\n",
    "\n",
    "In any case, now the network is HE-compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb0fa-2122-46d3-8dcf-dc528959da8c",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "From the applicative point of view, we have two options on how we want our Torch model to run on encrypted values:\n",
    "  1. Modify Torch layers code in order to be fully compatible with arrays of Pyfhel ciphertexts/encoded values;\n",
    "  2. Create the code for the general blocks of LeNet-1 (convolutional layer, linear layer, square layer, flatten...)\n",
    "  \n",
    "We opt for the second path, having already done this in our previous work: https://github.com/AlexMV12/PyCrCNN\n",
    "\n",
    "Let's remember that, in order to be used with the encrypted values, also the weights of the models will have to be **encoded**. This means that each value in the weights of each layer will be encoded in a polynomial.\n",
    "\n",
    "First, we define some useful functions to encrypt/encode matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf13fcad-5ac0-4a00-8ad6-43fbf0e731c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.encodeFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([encode_matrix(HE, m) for m in matrix])\n",
    "\n",
    "\n",
    "def decode_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.decodeFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([decode_matrix(HE, m) for m in matrix])\n",
    "\n",
    "\n",
    "def encrypt_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.encryptFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([encrypt_matrix(HE, m) for m in matrix])\n",
    "\n",
    "\n",
    "def decrypt_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.decryptFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([decrypt_matrix(HE, m) for m in matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c4f49-c53e-484b-9f6a-2930aea4cba0",
   "metadata": {},
   "source": [
    "Then, the actual code for the convolutional, linear, square, flatten and average pooling layer is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dbe6430-31e5-4528-aa68-f7131cea4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    def __init__(self, HE, weights, stride=(1, 1), padding=(0, 0), bias=None):\n",
    "        self.HE = HE\n",
    "        self.weights = encode_matrix(HE, weights)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        if bias is not None:\n",
    "            self.bias = encode_matrix(HE, bias)\n",
    "\n",
    "    def __call__(self, t):\n",
    "        t = apply_padding(t, self.padding)\n",
    "        result = np.array([[np.sum([convolute2d(image_layer, filter_layer, self.stride)\n",
    "                                    for image_layer, filter_layer in zip(image, _filter)], axis=0)\n",
    "                            for _filter in self.weights]\n",
    "                           for image in t])\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return np.array([[layer + bias for layer, bias in zip(image, self.bias)] for image in result])\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "def convolute2d(image, filter_matrix, stride):\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "    x_f = len(filter_matrix[0])\n",
    "    y_f = len(filter_matrix)\n",
    "\n",
    "    y_stride = stride[0]\n",
    "    x_stride = stride[1]\n",
    "\n",
    "    x_o = ((x_d - x_f) // x_stride) + 1\n",
    "    y_o = ((y_d - y_f) // y_stride) + 1\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_stride\n",
    "        index_column = x * x_stride\n",
    "        return matrix[index_row: index_row + y_f, index_column: index_column + x_f]\n",
    "\n",
    "    return np.array(\n",
    "        [[np.sum(get_submatrix(image, x, y) * filter_matrix) for x in range(0, x_o)] for y in range(0, y_o)])\n",
    "\n",
    "def apply_padding(t, padding):\n",
    "    y_p = padding[0]\n",
    "    x_p = padding[1]\n",
    "    zero = t[0][0][y_p+1][x_p+1] - t[0][0][y_p+1][x_p+1]\n",
    "    return [[np.pad(mat, ((y_p, y_p), (x_p, x_p)), 'constant', constant_values=zero) for mat in layer] for layer in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d80ce96f-22fe-4256-890c-cd752893b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, HE, weights, bias=None):\n",
    "        self.HE = HE\n",
    "        self.weights = encode_matrix(HE, weights)\n",
    "        self.bias = bias\n",
    "        if bias is not None:\n",
    "            self.bias = encode_matrix(HE, bias)\n",
    "\n",
    "    def __call__(self, t):\n",
    "        result = np.array([[np.sum(image * row) for row in self.weights] for image in t])\n",
    "        if self.bias is not None:\n",
    "            result = np.array([row + self.bias for row in result])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85a3370e-b2e4-4eac-89fc-2071f41aff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareLayer:\n",
    "    def __init__(self, HE):\n",
    "        self.HE = HE\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return square(self.HE, image)\n",
    "\n",
    "\n",
    "def square(HE, image):\n",
    "    try:\n",
    "        return np.array(list(map(lambda x: HE.power(x, 2), image)))\n",
    "    except TypeError:\n",
    "        return np.array([square(HE, m) for m in image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28b4e4e2-c07e-447a-b427-5140808177ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def __call__(self, image):\n",
    "        dimension = image.shape\n",
    "        return image.reshape(dimension[0], dimension[1]*dimension[2]*dimension[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13b04196-0347-436e-8f7f-169d5d0941ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePoolLayer:\n",
    "    def __init__(self, HE, kernel_size, stride=(1, 1), padding=(0, 0)):\n",
    "        self.HE = HE\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, t):\n",
    "        t = apply_padding(t, self.padding)\n",
    "        return np.array([[_avg(self.HE, layer, self.kernel_size, self.stride) for layer in image] for image in t])\n",
    "\n",
    "\n",
    "def _avg(HE, image, kernel_size, stride):\n",
    "    x_s = stride[1]\n",
    "    y_s = stride[0]\n",
    "\n",
    "    x_k = kernel_size[1]\n",
    "    y_k = kernel_size[0]\n",
    "\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "\n",
    "    x_o = ((x_d - x_k) // x_s) + 1\n",
    "    y_o = ((y_d - y_k) // y_s) + 1\n",
    "\n",
    "    denominator = HE.encodeFrac(1 / (x_k * y_k))\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_s\n",
    "        index_column = x * x_s\n",
    "        return matrix[index_row: index_row + y_k, index_column: index_column + x_k]\n",
    "\n",
    "    return [[np.sum(get_submatrix(image, x, y)) * denominator for x in range(0, x_o)] for y in range(0, y_o)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfd4c8-79bb-4647-9353-5f76629f1f56",
   "metadata": {},
   "source": [
    "We can now define a function to \"convert\" a PyTorch model to a list of sequential HE-ready-to-be-used layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "886fc91b-7301-4a31-830a-05b326745c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_pytorch(HE, net):\n",
    "    # Define builders for every possible layer\n",
    "\n",
    "    def conv_layer(layer):\n",
    "        if layer.bias is None:\n",
    "            bias = None\n",
    "        else:\n",
    "            bias = layer.bias.detach().numpy()\n",
    "\n",
    "        return ConvolutionalLayer(HE, weights=layer.weight.detach().numpy(),\n",
    "                                  stride=layer.stride,\n",
    "                                  padding=layer.padding,\n",
    "                                  bias=bias)\n",
    "\n",
    "    def lin_layer(layer):\n",
    "        if layer.bias is None:\n",
    "            bias = None\n",
    "        else:\n",
    "            bias = layer.bias.detach().numpy()\n",
    "        return LinearLayer(HE, layer.weight.detach().numpy(),\n",
    "                           bias)\n",
    "\n",
    "    def avg_pool_layer(layer):\n",
    "        # This proxy is required because in PyTorch an AvgPool2d can have kernel_size, stride and padding either of\n",
    "        # type (int, int) or int, unlike in Conv2d\n",
    "        kernel_size = (layer.kernel_size, layer.kernel_size) if isinstance(layer.kernel_size, int) else layer.kernel_size\n",
    "        stride = (layer.stride, layer.stride) if isinstance(layer.stride, int) else layer.stride\n",
    "        padding = (layer.padding, layer.padding) if isinstance(layer.padding, int) else layer.padding\n",
    "\n",
    "        return AveragePoolLayer(HE, kernel_size, stride, padding)\n",
    "\n",
    "    def flatten_layer(layer):\n",
    "        return FlattenLayer()\n",
    "\n",
    "    def square_layer(layer):\n",
    "        return SquareLayer(HE)\n",
    "\n",
    "    # Maps every PyTorch layer type to the correct builder\n",
    "    options = {\"Conv\": conv_layer,\n",
    "               \"Line\": lin_layer,\n",
    "               \"Flat\": flatten_layer,\n",
    "               \"AvgP\": avg_pool_layer,\n",
    "               \"Squa\": square_layer\n",
    "               }\n",
    "\n",
    "    encoded_layers = [options[str(layer)[0:4]](layer) for layer in net]\n",
    "    return encoded_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bde28e-7adc-4ede-876a-b5e57b63b0c4",
   "metadata": {},
   "source": [
    "## Encrypted processing\n",
    "\n",
    "Let's list the activities that we will now do:\n",
    "  1. Create a HE context, specifiying the encryption parameters `m` (polynomial modulus degree) and `p` (plaintext modulus). Let's remember that `q` will be chosen automatically in order to guarantee a 128-bit RSA equivalent security;\n",
    "  2. Convert our Torch approximated model to a list of layers able to work on matrices of encrypted values. The weights will be encoded;\n",
    "  3. Encrypt an image from our testing set;\n",
    "  4. Verify that the final classification result is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64023d-9b74-44c0-90a1-f6e7e0e3d123",
   "metadata": {},
   "source": [
    "If we look at our model, we can see that we have two **square layers**: these are the layers which have more impact on our noise!\n",
    "Two square layers corresponds to two ciphertext-ciphertext multiplications. Let's see if $m=4096$ gives us enough room to perform 2 encrypted multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6973b261-5b7e-462b-85e8-dc4396985eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "HE = Pyfhel()\n",
    "HE.contextGen(p=65538, m=4096, fracDigits=64)  # By default, fracDigits=32. Let's increase it to have more precision.\n",
    "HE.keyGen()\n",
    "relinKeySize=3\n",
    "HE.relinKeyGen(bitCount=2, size=relinKeySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1547ba5-bca5-4ab5-83fe-21de59d89404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mult 1 [budget: 55 dB]: 10.0 (expected 10)\n",
      "Mult 2 [budget: 26 dB]: 1.0 (expected 1.0)\n",
      "Mult 3 [budget: 0 dB]: -4.611686018427388e+18 (expected 10.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HE.multDepth(max_depth=64, delta=0.1, x_y_z=(1, 10, 0.1), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce31c0-31ee-47f6-9087-752684d08626",
   "metadata": {},
   "source": [
    "We are near the limit, so we have to change parameters. The simplest thing we can do is to increment $p$ and $m$: this will give us both room for noise and accuracy, at the cost of an heavier computation. Let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cad246c-65d9-4ffc-9c07-cdb7ce85bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HE = Pyfhel()\n",
    "HE.contextGen(p=655379848191252, m=8192, fracDigits=64)\n",
    "HE.keyGen()\n",
    "relinKeySize=3\n",
    "HE.relinKeyGen(bitCount=5, size=relinKeySize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c7148a1-91cf-4c47-b4e8-271407f67b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mult 1 [budget: 96 dB]: 10.0 (expected 10)\n",
      "Mult 2 [budget: 32 dB]: 1.0 (expected 1.0)\n",
      "Mult 3 [budget: 0 dB]: -6.862910432709034e+18 (expected 10.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HE.multDepth(max_depth=64, delta=0.1, x_y_z=(1, 10, 0.1), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1270f-e58d-4ed3-a71e-f63f27502eba",
   "metadata": {},
   "source": [
    "Now they should be okay! Let's try to encode our model, encrypt an image and compare the results with the expected ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32461748-5d18-4701-8eab-a023066e2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "sample_image = images[0]\n",
    "sample_label = labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26a402a6-0124-448c-b808-e85b49b390cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 420 ms, sys: 71.7 ms, total: 492 ms\n",
      "Wall time: 500 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LeNet1_Approx.to(\"cpu\")\n",
    "LeNet1_Encoded = build_from_pytorch(HE, LeNet1_Approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501ed35-d6d5-448d-b765-e9be2e0c8841",
   "metadata": {},
   "source": [
    "What is the expected output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58571823-05aa-44f2-beeb-e5bfbeac115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    expected_output = LeNet1_Approx(sample_image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95e18143-b143-4416-b38c-2a7ef673b2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-12.7048, -10.2085,  10.9441,   4.7339, -29.6800, -23.8598, -14.4046,\n",
       "         -11.8167,  -2.7060, -13.3006]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2666bb-87e1-4fd5-a85a-91f6f23a3ba4",
   "metadata": {},
   "source": [
    "Let's try encrypting the image and passing it through our encoded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d787fc06-60cf-4eb6-beab-59882f8f65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_image = encrypt_matrix(HE, sample_image.unsqueeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ede9425-9ef7-480f-a8ba-22e06a833477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed layer <__main__.ConvolutionalLayer object at 0x7fda4d6fc220>...\n",
      "Passed layer <__main__.SquareLayer object at 0x7fda4d6fc4c0>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7fda4d6fcdc0>...\n",
      "Passed layer <__main__.ConvolutionalLayer object at 0x7fda4d701070>...\n",
      "Passed layer <__main__.SquareLayer object at 0x7fda4c58e580>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7fda4d7019d0>...\n",
      "Passed layer <__main__.FlattenLayer object at 0x7fda4d701940>...\n",
      "Passed layer <__main__.LinearLayer object at 0x7fda4d6fe5b0>...\n",
      "Finished.\n",
      "CPU times: user 10min 3s, sys: 1.84 s, total: 10min 5s\n",
      "Wall time: 10min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for layer in LeNet1_Encoded:\n",
    "    encrypted_image = layer(encrypted_image)\n",
    "    print(f\"Passed layer {layer}...\")\n",
    "    \n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e71c85-7b94-40fb-a2e1-e31d37ddda65",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "Let's check the result: we can see the difference with respect to the computation in plain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4ced1c0-17e6-481d-8087-0e3798a69423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-12.7047981  -10.20841949  10.94171922   4.72890981 -29.67010898\n",
      "  -23.8592446  -14.40442349 -11.81639633  -2.70542545 -13.29817405]]\n"
     ]
    }
   ],
   "source": [
    "result = decrypt_matrix(HE, encrypted_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3aa3beab-1307-4358-ac70-39dd950c4b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.58617500e-05 -6.42027431e-05  2.38115966e-03  4.95068368e-03\n",
      "  -9.88179012e-03 -5.91982557e-04 -1.35647673e-04 -2.96978924e-04\n",
      "  -6.06827379e-04 -2.47368548e-03]]\n"
     ]
    }
   ],
   "source": [
    "difference = expected_output.numpy() - result\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb43bd-ec62-4c10-b8ce-e83e0f58a4f7",
   "metadata": {},
   "source": [
    "We are happy with the precision of this result: the difference between the expected output (obtained running the model in plain, on plain data) and the output obtained with encrypted processing is very low.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c1027-4592-43f6-bde3-34d3118789e3",
   "metadata": {},
   "source": [
    "### Computational load\n",
    "Obviously, we cannot ignore the huge computational overhead generated by the encrypted processing.\n",
    "\n",
    "In fact, the processing of one image took about ~10min on a common desktop machine.\n",
    "The computation has not been parallelized; so, it used only one thread.\n",
    "\n",
    "While parallelizing allows to speed up the computation, also the occupied memory is a concern: the processing of this image occupied ~5GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d429c-1dfb-4cb4-8e06-59091e0083c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timex_alkemy",
   "language": "python",
   "name": "timex_alkemy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
