{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37db485d-fcec-4c20-943e-239a74a70cf4",
   "metadata": {},
   "source": [
    "# Homomorphic Encrypted LeNet-1\n",
    "This notebook will show a very practical example of running the famous LeNet-1 DL model directly on encrypted data.\n",
    "\n",
    "![scheme](HE_processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6b3e7-d428-48e4-b9db-3609ceb94b25",
   "metadata": {},
   "source": [
    "## Homomorphic encryption operations\n",
    "First of all, we will look at Pyfhel, a Python library which wraps SEAL, one of the most used frameworks for HE.\n",
    "Pyfhel supports the BFV scheme, so, it is the one that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c74dbc3-b890-4a87-9e8e-0094b9fc17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pyfhel obj at 0x7f0de4174b50, [pk:Y, sk:Y, rtk:-, rlk:-, contx(p=65537, m=4096, base=2, sec=128, dig=64i.32f, batch=False)]>\n",
      "Expected sum: 125.028207448, decrypted sum: 125.02820744784549\n",
      "Expected sub: 129.286137812, decrypted sum: 129.28613781183958\n",
      "Expected mul: -270.7131931708334, decrypted sum: -270.7131931686308\n"
     ]
    }
   ],
   "source": [
    "from Pyfhel import Pyfhel, PyPtxt, PyCtxt\n",
    "\n",
    "HE = Pyfhel()\n",
    "HE.contextGen(p=65537, m=4096)\n",
    "HE.keyGen()\n",
    "\n",
    "print(HE)\n",
    "\n",
    "a = 127.15717263\n",
    "b = -2.128965182\n",
    "ctxt1 = HE.encryptFrac(a)\n",
    "ctxt2 = HE.encryptFrac(b)\n",
    "\n",
    "ctxtSum = ctxt1 + ctxt2\n",
    "ctxtSub = ctxt1 - ctxt2\n",
    "ctxtMul = ctxt1 * ctxt2\n",
    "\n",
    "resSum = HE.decryptFrac(ctxtSum)\n",
    "resSub = HE.decryptFrac(ctxtSub) \n",
    "resMul = HE.decryptFrac(ctxtMul)\n",
    "\n",
    "print(f\"Expected sum: {a+b}, decrypted sum: {resSum}\")\n",
    "print(f\"Expected sub: {a-b}, decrypted sum: {resSub}\")\n",
    "print(f\"Expected mul: {a*b}, decrypted sum: {resMul}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274ad1bb-c26e-4f29-9301-997c04f43bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "m1 = HE.encryptFrac(a)\n",
    "print(HE.noiseLevel(m1))\n",
    "\n",
    "m2 = HE.encodeFrac(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09497da-60be-4c1f-9d2a-9bd2b9390e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "54\n",
      "82\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "print(HE.noiseLevel(m1+m1))\n",
    "print(HE.noiseLevel(m1*m1))\n",
    "print(HE.noiseLevel(m1+m2))\n",
    "print(HE.noiseLevel(m1*m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d0840-41a8-417a-97db-faab642806d6",
   "metadata": {},
   "source": [
    "Before starting, let's note that:\n",
    "  1. We will use the fractional encoder to encode (and encrypt) the values in our examples. BFV was born for integers, so, CKKS should be used if the use case involves fractional values. However it is a more complex scheme, and for this example BFV is sufficient.\n",
    "  2. We will not use batching (also called *packing*). While batching can greatly speed up the computations, it introduces limitations which make the encrypted ML much more complex. For this example, we will encrypt/encode each number with a polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b17e-7644-4ce8-99a6-c1906758e2eb",
   "metadata": {},
   "source": [
    "## LeNet-1\n",
    "The LeNet-1 is a small CNN developed by LeCun et al. It is composed of 5 layers: a convolutional layer with 4 kernels of size 5x5 and tanh activation, an average pooling layer with kernel of size 2, another convolutional layer with 16 kernels of size 5x5 and tanh activation, another average pooling layer with kernel of size 2, and a fully connected layers with size 192x10. \n",
    "\n",
    "The highest value in the output tensor corresponds to the label LeNet-1 associated to the input image. \n",
    "\n",
    "For this tutorial we will use the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc122e5-a460-4a73-bed7-012b1105081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89940a02-261f-4fc7-98e7-cea0a70be181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcf92dc-4929-4adc-b97e-2a1bf946bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6641a660-43a4-4ed4-895a-ac7d1841dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "def train_net(network, epochs, device):\n",
    "    optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader: # Get Batch\n",
    "            images, labels = batch \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            preds = network(images) # Pass Batch\n",
    "            loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # Calculate Gradients\n",
    "            optimizer.step() # Update Weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        \n",
    "def test_net(network, device):\n",
    "    network.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader: # Get Batch\n",
    "            images, labels = batch \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            preds = network(images) # Pass Batch\n",
    "            loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        accuracy = round(100. * (total_correct / len(test_loader.dataset)), 4)\n",
    "\n",
    "    return total_correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8f74067-df4d-4b70-ae4c-ec6970c8baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True # If set to false, it will load models previously trained and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a4f779-4a5f-4a56-94c1-dba733942fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8480f18-5742-45a6-b4b7-5839b2f96763",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    accuracies = []\n",
    "    for i in range(0, experiments):\n",
    "        LeNet1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(4, 12, kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(192, 10),\n",
    "        )\n",
    "        \n",
    "        LeNet1.to(device)\n",
    "        train_net(LeNet1, 15, device)\n",
    "        acc = test_net(LeNet1, device)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    torch.save(LeNet1, \"LeNet1.pt\")\n",
    "else:\n",
    "    LeNet1 = torch.load(\"LeNet1.pt\")\n",
    "    LeNet1.eval()\n",
    "    LeNet1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fbca30-aa89-4d74-a476-3c9107fbf851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on test set: 0.9882\n",
      "Var: 0.0\n"
     ]
    }
   ],
   "source": [
    "m = np.array(accuracies)\n",
    "print(f\"Mean accuracy on test set: {np.mean(m)}\")\n",
    "print(f\"Var: {np.var(m)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025dfbc9-38d0-41a7-a140-9606bfeff53f",
   "metadata": {},
   "source": [
    "## Approximating\n",
    "As we know, there are some operations that cannot be performed homomorphically on encrypted values. Most notably, these operations are division and comparison. It is possible to perform only linear functions.\n",
    "\n",
    "Consequently, in the LeNet-1 scheme we used, we can not use `tanh()`. This is because we cannot apply its non-linearities.\n",
    "\n",
    "\n",
    "One of the most common approach is to replace it with a simple polynomial function, for example a square layer (which simply performs $x \\rightarrow x^2$).\n",
    "\n",
    "We define the model with all the non-linearities removed **approximated**. This model can be re-trained, and it will be ready to be used on encrypted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52ea6449-1108-446f-ba2d-8170edcb641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, t):\n",
    "        return torch.pow(t, 2)\n",
    "\n",
    "LeNet1_Approx = nn.Sequential(\n",
    "    nn.Conv2d(1, 4, kernel_size=5),\n",
    "    Square(),\n",
    "    nn.AvgPool2d(kernel_size=2),\n",
    "            \n",
    "    nn.Conv2d(4, 12, kernel_size=5),\n",
    "    Square(),\n",
    "    nn.AvgPool2d(kernel_size=2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(192, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1ccaff6b-7cd5-49d2-9b5c-e8a881005fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    approx_accuracies = []\n",
    "    for i in range(0, experiments):\n",
    "        LeNet1_Approx = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            Square(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(4, 12, kernel_size=5),\n",
    "            Square(),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(192, 10),\n",
    "        )\n",
    "        \n",
    "        LeNet1_Approx.to(device)\n",
    "        train_net(LeNet1_Approx, 15, device)\n",
    "        acc = test_net(LeNet1_Approx, device)\n",
    "        approx_accuracies.append(acc)\n",
    "        \n",
    "    torch.save(LeNet1, \"LeNet1_Approx.pt\")\n",
    "\n",
    "else:\n",
    "    LeNet1_Approx = torch.load(\"LeNet1_Approx.pt\")\n",
    "    LeNet1_Approx.eval()\n",
    "    LeNet1_Approx.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5434fcf-e01c-40c8-9ea8-bc9086e8beef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.9889\n",
      "Var: 0.0\n"
     ]
    }
   ],
   "source": [
    "m = np.array(approx_accuracies)\n",
    "print(f\"Mean: {np.mean(m)}\")\n",
    "print(f\"Var: {np.var(m)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb96de6-7053-40ce-bb88-a1532270683f",
   "metadata": {},
   "source": [
    "We can see that replacing `tanh()` with `square()` did not impact the accuracy of the model dramatically. Usually this is not the case, and approximating DL models may worsen the performance badly. This is one of the challenges that HE-ML will have to consider: the creation of DL models keeping in mind the HE constraints from the beginning.\n",
    "\n",
    "In any case, now the network is HE-compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874fb0fa-2122-46d3-8dcf-dc528959da8c",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "From the applicative point of view, we have two options on how we want our Torch model to run on encrypted values:\n",
    "  1. Modify Torch layers code in order to be fully compatible with arrays of Pyfhel ciphertexts/encoded values;\n",
    "  2. Create the code for the general blocks of LeNet-1 (convolutional layer, linear layer, square layer, flatten...)\n",
    "  \n",
    "We opt for the second path, having already done this in our previous work: https://github.com/AlexMV12/PyCrCNN\n",
    "\n",
    "Let's remember that, in order to be used with the encrypted values, also the weights of the models will have to be **encoded**. This means that each value in the weights of each layer will be encoded in a polynomial.\n",
    "\n",
    "First, we define some useful functions to encrypt/encode matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf13fcad-5ac0-4a00-8ad6-43fbf0e731c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.encodeFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([encode_matrix(HE, m) for m in matrix])\n",
    "\n",
    "\n",
    "def decode_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.decodeFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([decode_matrix(HE, m) for m in matrix])\n",
    "\n",
    "\n",
    "def encrypt_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.encryptFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([encrypt_matrix(HE, m) for m in matrix])\n",
    "\n",
    "\n",
    "def decrypt_matrix(HE, matrix):\n",
    "    try:\n",
    "        return np.array(list(map(HE.decryptFrac, matrix)))\n",
    "    except TypeError:\n",
    "        return np.array([decrypt_matrix(HE, m) for m in matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c4f49-c53e-484b-9f6a-2930aea4cba0",
   "metadata": {},
   "source": [
    "Then, the actual code for the convolutional, linear, square, flatten and average pooling layer is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4dbe6430-31e5-4528-aa68-f7131cea4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer:\n",
    "    def __init__(self, HE, weights, stride=(1, 1), padding=(0, 0), bias=None):\n",
    "        self.HE = HE\n",
    "        self.weights = encode_matrix(HE, weights)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = bias\n",
    "        if bias is not None:\n",
    "            self.bias = encode_matrix(HE, bias)\n",
    "\n",
    "    def __call__(self, t):\n",
    "        t = apply_padding(t, self.padding)\n",
    "        result = np.array([[np.sum([convolute2d(image_layer, filter_layer, self.stride)\n",
    "                                    for image_layer, filter_layer in zip(image, _filter)], axis=0)\n",
    "                            for _filter in self.weights]\n",
    "                           for image in t])\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return np.array([[layer + bias for layer, bias in zip(image, self.bias)] for image in result])\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "def convolute2d(image, filter_matrix, stride):\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "    x_f = len(filter_matrix[0])\n",
    "    y_f = len(filter_matrix)\n",
    "\n",
    "    y_stride = stride[0]\n",
    "    x_stride = stride[1]\n",
    "\n",
    "    x_o = ((x_d - x_f) // x_stride) + 1\n",
    "    y_o = ((y_d - y_f) // y_stride) + 1\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_stride\n",
    "        index_column = x * x_stride\n",
    "        return matrix[index_row: index_row + y_f, index_column: index_column + x_f]\n",
    "\n",
    "    return np.array(\n",
    "        [[np.sum(get_submatrix(image, x, y) * filter_matrix) for x in range(0, x_o)] for y in range(0, y_o)])\n",
    "\n",
    "def apply_padding(t, padding):\n",
    "    y_p = padding[0]\n",
    "    x_p = padding[1]\n",
    "    zero = t[0][0][y_p+1][x_p+1] - t[0][0][y_p+1][x_p+1]\n",
    "    return [[np.pad(mat, ((y_p, y_p), (x_p, x_p)), 'constant', constant_values=zero) for mat in layer] for layer in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d80ce96f-22fe-4256-890c-cd752893b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, HE, weights, bias=None):\n",
    "        self.HE = HE\n",
    "        self.weights = encode_matrix(HE, weights)\n",
    "        self.bias = bias\n",
    "        if bias is not None:\n",
    "            self.bias = encode_matrix(HE, bias)\n",
    "\n",
    "    def __call__(self, t):\n",
    "        result = np.array([[np.sum(image * row) for row in self.weights] for image in t])\n",
    "        if self.bias is not None:\n",
    "            result = np.array([row + self.bias for row in result])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85a3370e-b2e4-4eac-89fc-2071f41aff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareLayer:\n",
    "    def __init__(self, HE):\n",
    "        self.HE = HE\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return square(self.HE, image)\n",
    "\n",
    "\n",
    "def square(HE, image):\n",
    "    try:\n",
    "        return np.array(list(map(lambda x: HE.power(x, 2), image)))\n",
    "    except TypeError:\n",
    "        return np.array([square(HE, m) for m in image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28b4e4e2-c07e-447a-b427-5140808177ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    def __call__(self, image):\n",
    "        dimension = image.shape\n",
    "        return image.reshape(dimension[0], dimension[1]*dimension[2]*dimension[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13b04196-0347-436e-8f7f-169d5d0941ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePoolLayer:\n",
    "    def __init__(self, HE, kernel_size, stride=(1, 1), padding=(0, 0)):\n",
    "        self.HE = HE\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, t):\n",
    "        t = apply_padding(t, self.padding)\n",
    "        return np.array([[_avg(self.HE, layer, self.kernel_size, self.stride) for layer in image] for image in t])\n",
    "\n",
    "\n",
    "def _avg(HE, image, kernel_size, stride):\n",
    "    x_s = stride[1]\n",
    "    y_s = stride[0]\n",
    "\n",
    "    x_k = kernel_size[1]\n",
    "    y_k = kernel_size[0]\n",
    "\n",
    "    x_d = len(image[0])\n",
    "    y_d = len(image)\n",
    "\n",
    "    x_o = ((x_d - x_k) // x_s) + 1\n",
    "    y_o = ((y_d - y_k) // y_s) + 1\n",
    "\n",
    "    denominator = HE.encodeFrac(1 / (x_k * y_k))\n",
    "\n",
    "    def get_submatrix(matrix, x, y):\n",
    "        index_row = y * y_s\n",
    "        index_column = x * x_s\n",
    "        return matrix[index_row: index_row + y_k, index_column: index_column + x_k]\n",
    "\n",
    "    return [[np.sum(get_submatrix(image, x, y)) * denominator for x in range(0, x_o)] for y in range(0, y_o)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfd4c8-79bb-4647-9353-5f76629f1f56",
   "metadata": {},
   "source": [
    "We can now define a function to \"convert\" a PyTorch model to a list of sequential HE-ready-to-be-used layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "886fc91b-7301-4a31-830a-05b326745c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_pytorch(HE, net):\n",
    "    # Define builders for every possible layer\n",
    "\n",
    "    def conv_layer(layer):\n",
    "        if layer.bias is None:\n",
    "            bias = None\n",
    "        else:\n",
    "            bias = layer.bias.detach().numpy()\n",
    "\n",
    "        return ConvolutionalLayer(HE, weights=layer.weight.detach().numpy(),\n",
    "                                  stride=layer.stride,\n",
    "                                  padding=layer.padding,\n",
    "                                  bias=bias)\n",
    "\n",
    "    def lin_layer(layer):\n",
    "        if layer.bias is None:\n",
    "            bias = None\n",
    "        else:\n",
    "            bias = layer.bias.detach().numpy()\n",
    "        return LinearLayer(HE, layer.weight.detach().numpy(),\n",
    "                           bias)\n",
    "\n",
    "    def avg_pool_layer(layer):\n",
    "        # This proxy is required because in PyTorch an AvgPool2d can have kernel_size, stride and padding either of\n",
    "        # type (int, int) or int, unlike in Conv2d\n",
    "        kernel_size = (layer.kernel_size, layer.kernel_size) if isinstance(layer.kernel_size, int) else layer.kernel_size\n",
    "        stride = (layer.stride, layer.stride) if isinstance(layer.stride, int) else layer.stride\n",
    "        padding = (layer.padding, layer.padding) if isinstance(layer.padding, int) else layer.padding\n",
    "\n",
    "        return AveragePoolLayer(HE, kernel_size, stride, padding)\n",
    "\n",
    "    def flatten_layer(layer):\n",
    "        return FlattenLayer()\n",
    "\n",
    "    def square_layer(layer):\n",
    "        return SquareLayer(HE)\n",
    "\n",
    "    # Maps every PyTorch layer type to the correct builder\n",
    "    options = {\"Conv\": conv_layer,\n",
    "               \"Line\": lin_layer,\n",
    "               \"Flat\": flatten_layer,\n",
    "               \"AvgP\": avg_pool_layer,\n",
    "               \"Squa\": square_layer\n",
    "               }\n",
    "\n",
    "    encoded_layers = [options[str(layer)[0:4]](layer) for layer in net]\n",
    "    return encoded_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bde28e-7adc-4ede-876a-b5e57b63b0c4",
   "metadata": {},
   "source": [
    "## Encrypted processing\n",
    "\n",
    "Let's list the activities that we will now do:\n",
    "  1. Create a HE context, specifiying the encryption parameters `m` (polynomial modulus degree) and `p` (plaintext modulus). Let's remember that `q` will be chosen automatically in order to guarantee a 128-bit RSA equivalent security;\n",
    "  2. Convert our Torch approximated model to a list of layers able to work on matrices of encrypted values. The weights will be encoded;\n",
    "  3. Encrypt an image from our testing set;\n",
    "  4. Verify that the final classification result is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64023d-9b74-44c0-90a1-f6e7e0e3d123",
   "metadata": {},
   "source": [
    "If we look at our model, we can see that we have two **square layers**: these are the layers which have more impact on our noise!\n",
    "Two square layers corresponds to two ciphertext-ciphertext multiplications. Let's see if $m=4096$ gives us enough room to perform 2 encrypted multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a03f8f-997e-4009-94b4-541f7cc56318",
   "metadata": {},
   "source": [
    "We can define a function which, after receiving $n$ and $p$ tries to encrypt and image and forward it to our approximated model (suitable encoded). This will let us see the homomorphic encryption in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3cb6d3e9-072b-4645-97da-8e0eab92e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_parameters(n, p, model):\n",
    "    HE = Pyfhel()\n",
    "    HE.contextGen(p=p, m=n) # what Pyfhel calls m, we call n.\n",
    "    HE.keyGen()\n",
    "    relinKeySize=3\n",
    "    HE.relinKeyGen(bitCount=2, size=relinKeySize)\n",
    "    \n",
    "    images, labels = next(iter(test_loader))\n",
    "\n",
    "    sample_image = images[0]\n",
    "    sample_label = labels[0]\n",
    "    \n",
    "    model.to(\"cpu\")\n",
    "    model_encoded = build_from_pytorch(HE, model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        expected_output = model(sample_image.unsqueeze(0))\n",
    "    \n",
    "    encrypted_image = encrypt_matrix(HE, sample_image.unsqueeze(0).numpy())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for layer in model_encoded:\n",
    "        encrypted_image = layer(encrypted_image)\n",
    "        print(f\"Passed layer {layer}...\")\n",
    "    \n",
    "    requested_time = round(time.time() - start_time, 2)\n",
    "    \n",
    "    result = decrypt_matrix(HE, encrypted_image)\n",
    "    difference = expected_output.numpy() - result\n",
    "    \n",
    "    print(f\"\\nThe encrypted processing of one image requested {requested_time} seconds.\")\n",
    "    print(f\"\\nThe expected result was:\")\n",
    "    print(expected_output)\n",
    "    \n",
    "    print(f\"\\nThe actual result is: \")\n",
    "    print(result)\n",
    "    \n",
    "    print(f\"\\nThe error is:\")\n",
    "    print(difference)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468be664-a5f5-4c11-ad4f-4b2b5a06f46b",
   "metadata": {},
   "source": [
    "Let's try with $n=4096$ and $p=65536$ on our approximated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0515705e-71a0-44c7-a6ab-00e2c825aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed layer <__main__.ConvolutionalLayer object at 0x7f0d3d651820>...\n",
      "Passed layer <__main__.SquareLayer object at 0x7f0d3f110850>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7f0d3f110ee0>...\n",
      "Passed layer <__main__.ConvolutionalLayer object at 0x7f0d3f110ca0>...\n",
      "Passed layer <__main__.SquareLayer object at 0x7f0d3f110eb0>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7f0d3f1101c0>...\n",
      "Passed layer <__main__.FlattenLayer object at 0x7f0d3f110880>...\n",
      "Passed layer <__main__.LinearLayer object at 0x7f0d3f110310>...\n",
      "\n",
      "The encrypted processing of one image requested 148.9 seconds.\n",
      "\n",
      "The expected result was:\n",
      "tensor([[ -8.9517,   2.1390,  -5.7292, -15.0571,  -6.9279, -10.5527,  -8.9003,\n",
      "          -3.8584,  -4.2362, -10.4316]])\n",
      "\n",
      "The actual result is: \n",
      "[[-0.46503287  0.21019887  0.0091486  -1.42267955 -0.37975147 -0.45589089\n",
      "  -0.00624331 -0.05148689 -0.63904862 -1.1372307 ]]\n",
      "\n",
      "The error is:\n",
      "[[ -8.48666639   1.92882038  -5.73832035 -13.63442551  -6.54817841\n",
      "  -10.09681296  -8.89401094  -3.80686529  -3.59715172  -9.29434031]]\n"
     ]
    }
   ],
   "source": [
    "test_parameters(4096, 65536, LeNet1_Approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb43bd-ec62-4c10-b8ce-e83e0f58a4f7",
   "metadata": {},
   "source": [
    "Unfortunately, the NB is not sufficient and the decryption fails.\n",
    "We could try decrementing $p$, in order to reduce the NB consumption. However, the actual value is already quite low so it is difficult that it will change something.\n",
    "We could try incrementing $n$ to 8192, but it would result in a huge overhead in terms of memory and time.\n",
    "\n",
    "For now, it is simpler to remove a square layer from the DL model (usually removing the last one is better). This will result in a less NB demanding processing, allowing us to use $n=4096$.\n",
    "\n",
    "\n",
    "**Note that this does not happen on - every - image, but test a few and you will see that it happens often, making the computation unreliable. It means that we are very near the limit of NB. However, even if decryption does not fail, the results are quite unaccurate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "00e2c386-3a0f-4a1d-aeec-3d6d9733e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set (single square layer): 0.9827\n"
     ]
    }
   ],
   "source": [
    "LeNet1_Approx_singlesquare = nn.Sequential(\n",
    "    nn.Conv2d(1, 4, kernel_size=5),\n",
    "    Square(),\n",
    "    nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "    nn.Conv2d(4, 12, kernel_size=5),\n",
    "#     Square(),\n",
    "    nn.AvgPool2d(kernel_size=2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(192, 10),\n",
    ")\n",
    "\n",
    "LeNet1_Approx_singlesquare.to(device)\n",
    "train_net(LeNet1_Approx_singlesquare, 15, device)\n",
    "acc = test_net(LeNet1_Approx_singlesquare, device)\n",
    "print(f\"Accuracy on test set (single square layer): {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a0b453-930e-40c4-9447-680c10772bf4",
   "metadata": {},
   "source": [
    "Let's try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "894b9b53-e285-4f3f-be90-ab1e51fc2432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed layer <__main__.ConvolutionalLayer object at 0x7f0d3d6513d0>...\n",
      "Passed layer <__main__.SquareLayer object at 0x7f0d3d651760>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7f0d3d651ca0>...\n",
      "Passed layer <__main__.ConvolutionalLayer object at 0x7f0d58654ca0>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7f0d586545b0>...\n",
      "Passed layer <__main__.FlattenLayer object at 0x7f0d58654cd0>...\n",
      "Passed layer <__main__.LinearLayer object at 0x7f0d58654610>...\n",
      "\n",
      "The encrypted processing of one image requested 135.65 seconds.\n",
      "\n",
      "The expected result was:\n",
      "tensor([[ -2.3643, -10.4025,  -9.4854,   8.1968, -13.4863,  23.9925,  -2.2109,\n",
      "          -9.5026,   3.1787,   6.5724]])\n",
      "\n",
      "The actual result is: \n",
      "[[-0.12954384  0.54565309  0.54853017 -0.28500836 -1.45542385  0.78199623\n",
      "  -0.38797344  1.00632607 -0.23414565 -0.98088525]]\n",
      "\n",
      "The error is:\n",
      "[[ -2.23472518 -10.94810842 -10.03392465   8.48180477 -12.03084037\n",
      "   23.21052506  -1.82297359 -10.50893819   3.41285397   7.55328582]]\n"
     ]
    }
   ],
   "source": [
    "test_parameters(4096, 65536, LeNet1_Approx_singlesquare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad2908-ac44-437d-80de-d146f2559758",
   "metadata": {},
   "source": [
    "Now the decryption works! However, the final result are similar to the expected one, but with a discrepancy.\n",
    "The \"perfect\" value for $p$ can be found with a trial and error process. For our purposes, it has been set to 953983721."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "58d01f5c-c3c2-4a89-b2f7-64aebbbdd39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed layer <__main__.ConvolutionalLayer object at 0x7f0d58648fa0>...\n",
      "Passed layer <__main__.SquareLayer object at 0x7f0d58654610>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7f0d586544f0>...\n",
      "Passed layer <__main__.ConvolutionalLayer object at 0x7f0d3d6518b0>...\n",
      "Passed layer <__main__.AveragePoolLayer object at 0x7f0d3d651730>...\n",
      "Passed layer <__main__.FlattenLayer object at 0x7f0d3d651c70>...\n",
      "Passed layer <__main__.LinearLayer object at 0x7f0d3d651760>...\n",
      "\n",
      "The encrypted processing of one image requested 136.9 seconds.\n",
      "\n",
      "The expected result was:\n",
      "tensor([[ 14.9175,  -9.0088,  -4.3928,   3.4706,  -8.4480,   4.0986,   7.3694,\n",
      "         -12.8007,   1.6725,   0.8914]])\n",
      "\n",
      "The actual result is: \n",
      "[[ 14.88360887  -9.00425966  -4.39821204   3.46483131  -8.40817041\n",
      "    4.07068493   7.36104876 -12.79374951   1.67553707   0.89423412]]\n",
      "\n",
      "The error is:\n",
      "[[ 0.03386778 -0.00449126  0.00543603  0.00572877 -0.03981619  0.02789209\n",
      "   0.00833744 -0.00692779 -0.00305124 -0.00278729]]\n"
     ]
    }
   ],
   "source": [
    "test_parameters(4096, 953983721, LeNet1_Approx_singlesquare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf92d0c-b70c-45a6-8086-08cc5c21e449",
   "metadata": {},
   "source": [
    "We are now happy with the precision of the result, which should now guarantee that the final accuracy of the encrypted processing on the whole test set is equal (or, at least, very similar) to the same obtained on unencrypted data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c1027-4592-43f6-bde3-34d3118789e3",
   "metadata": {},
   "source": [
    "### Computational load\n",
    "Obviously, we cannot ignore the huge computational overhead generated by the encrypted processing.\n",
    "\n",
    "In fact, the processing of one image took about ~2min on a common desktop machine.\n",
    "The computation has not been parallelized; so, it used only one thread.\n",
    "\n",
    "While parallelizing allows to speed up the computation, also the occupied memory is a concern: the processing of this image occupied ~700MB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d429c-1dfb-4cb4-8e06-59091e0083c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c04ca-3ec4-4c0a-96f1-5d82801101b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyfhel",
   "language": "python",
   "name": "pyfhel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
